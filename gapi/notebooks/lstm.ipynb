{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "ein.tags": "worksheet-0",
                "slideshow": {
                    "slide_type": "-"
                }
            },
            "source": [
                "# LSTM Optimization with Groq API"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "ein.tags": "worksheet-0",
                "slideshow": {
                    "slide_type": "-"
                }
            },
            "source": [
                "In this notebook, we will:\n",
                " - Describe an LSTM and create a NumPy baseline model to compare against\n",
                " - Design reusable components for LSTM sub-kernels\n",
                " - Design an optimized LSTM in Groq API\n",
                " - Visualize and analyze the performance results with GroqView\n",
                " - Compare the results with state-of-the-art performances."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## LSTM Introduction\n",
                "\n",
                "Long Short-Term Memory (LSTM) models are a class of recurrent neural network (RNN). \n",
                "\n",
                "![LSTM dataflow graph](img/lstm-dag.png)\n",
                "\n",
                "[Source: Microsoft Brainwave architecture ISCA paper](https://www.microsoft.com/en-us/research/uploads/prod/2018/06/ISCA18-Brainwave-CameraReady.pdf)\n",
                "\n",
                "The figure above shows the compute graph for a single LSTM layer. We can observe a few traits:\n",
                " - The primary compute kernel is matrix-vector multiplication (MVM)\n",
                "   - MVMs have poor data reuse, which makes this an SRAM bandwidth-bound problem.\n",
                "   - We should be able to get strong batching performance on the GroqChipâ„¢ processor by packing multiple activation vectors into a matrix.\n",
                " - There is high potential for VXM chaining\n",
                "   - Few tensors must be written to memory.\n",
                "   - Many tensors are only consumed once."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LSTM Optimization Plan\n",
                "The follow diagram shows our plan for mapping the LSTM cell on to Groq API components. \n",
                "\n",
                "We will define 4 major components:\n",
                "\n",
                "1. MVMQuad: 4x concurrent matrix-vector multiplication (MVM) ops, one per LSTM gate.\n",
                "1. DequantizeBias: Convert from the MXM output format, quantized `int32`, to `float32` then add the bias vector. Support 4x concurrent DequantizeBias operations.\n",
                "1. Activations: Apply an activation function, either `sigmoid` or `hyperbolic tangent` to each of the 4x streams from DequantizeBias. Support 2x concurrent activation operations.\n",
                "1. Reduction: Pipeline of vector operations that reduce the 4x streams from the activations into the next-state vectors `cnext` and `hnext`\n",
                "\n",
                "![LSTM Mapping](img/lstm_mapping.png)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "ein.tags": "worksheet-0",
                "slideshow": {
                    "slide_type": "-"
                }
            },
            "source": [
                "## Boilerplate Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "autoscroll": false,
                "ein.hycell": false,
                "ein.tags": "worksheet-0",
                "slideshow": {
                    "slide_type": "-"
                }
            },
            "outputs": [],
            "source": [
                "import groq.api as g\n",
                "from groq.runner import tsp\n",
                "from groq.api import nn\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Quantization\n",
                "\n",
                "The literature shows that LSTMs can maintain high accuracy using narrow precision data types. Given that our performance is SRAM bandwidth-bound, we want to maximize our effective bandwidth by minimizing the data type size. On GroqChip, we select the int8 data type for matmuls (and therefore weight and activation data) because it is the narrowest available type. VXM functions will remain in float32.\n",
                "\n",
                "We select a fixed point 1.7 scheme on top of int8 so that we can express fractional numbers, which is important in LSTMs because the sigmoid and tanh activation functions expect a narrow range of inputs. Ideally, the fixed point scheme is chosen based on calibration against real-world data.\n",
                "\n",
                "To accommodate our chosen data types we must:\n",
                " - Offline, quantize our weights and inputs to fixed point\n",
                " - Online, dequantize the int32 (fixed point 18.14) matmul results into float32 by:\n",
                "   - Cast int32 to float32\n",
                "   - Multiply by a scaling factor of 2^-14\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "quantize_scale = 2 ** 7\n",
                "dequantize_scale = 1 / (2 ** 14)\n",
                "\n",
                "# Quantize float32 to fixed point 1.7 format\n",
                "def quantize(tensor, scale):\n",
                "    tensor = tensor * scale\n",
                "    tensor = tensor.astype(np.int8)\n",
                "    return tensor\n",
                "\n",
                "# Dequantize fixed point 18.14 to float32\n",
                "def dequantize(tensor, scale):\n",
                "    tensor = tensor.astype(np.float32)\n",
                "    tensor = tensor * scale\n",
                "    return tensor"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Preparation\n",
                "Let's set up some randomized data for the activations, weights, bias, and constants. We can also perform offline quantization here. \n",
                "\n",
                "It is important to note that LSTMs are numerically fickle and it is important to carefully choose the range for our random initialization. The reason is that sigm() and tanh() expect a narrow range of input values and it is easy to accidentally saturate them. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "hidden_size = 640\n",
                "input_size = 640\n",
                "batch = 3\n",
                "\n",
                "w_shape = (input_size, hidden_size)\n",
                "u_shape = (hidden_size, hidden_size)\n",
                "w_shape_transposed = (w_shape[1], w_shape[0])\n",
                "u_shape_transposed = (u_shape[1], u_shape[0])\n",
                "x_shape = (batch, input_size)\n",
                "v_shape = (batch, hidden_size)\n",
                "\n",
                "in_concat_shape = (batch, input_size + hidden_size)\n",
                "mat_concat_shape = (input_size + hidden_size, hidden_size)\n",
                "mat_concat_transpose_shape = (mat_concat_shape[1], mat_concat_shape[0])\n",
                "\n",
                "x_data = quantize((-2) * np.random.random_sample(size=x_shape) + 1, quantize_scale)\n",
                "cprev_data = (-2) * np.random.random_sample(size=v_shape).astype(np.float32) + 1\n",
                "hprev_data = quantize(\n",
                "    (-2) * np.random.random_sample(size=v_shape).astype(np.float32) + 1,\n",
                "    quantize_scale,\n",
                ")\n",
                "\n",
                "bi_data = (-64) * np.random.random_sample(size=v_shape).astype(np.float32) + 32\n",
                "bf_data = (-64) * np.random.random_sample(size=v_shape).astype(np.float32) + 32\n",
                "bo_data = (-64) * np.random.random_sample(size=v_shape).astype(np.float32) + 32\n",
                "bc_data = (-64) * np.random.random_sample(size=v_shape).astype(np.float32) + 32\n",
                "\n",
                "Wi_data = quantize(\n",
                "    (-0.5) * np.random.random_sample(size=w_shape).astype(np.float32) + 0.25,\n",
                "    quantize_scale,\n",
                ")\n",
                "Wf_data = quantize(\n",
                "    (-0.5) * np.random.random_sample(size=w_shape).astype(np.float32) + 0.25,\n",
                "    quantize_scale,\n",
                ")\n",
                "Wo_data = quantize(\n",
                "    (-0.5) * np.random.random_sample(size=w_shape).astype(np.float32) + 0.25,\n",
                "    quantize_scale,\n",
                ")\n",
                "Wc_data = quantize(\n",
                "    (-0.5) * np.random.random_sample(size=w_shape).astype(np.float32) + 0.25,\n",
                "    quantize_scale,\n",
                ")\n",
                "\n",
                "Ui_data = quantize(\n",
                "    (-0.5) * np.random.random_sample(size=u_shape).astype(np.float32) + 0.25,\n",
                "    quantize_scale,\n",
                ")\n",
                "Uf_data = quantize(\n",
                "    (-0.5) * np.random.random_sample(size=u_shape).astype(np.float32) + 0.25,\n",
                "    quantize_scale,\n",
                ")\n",
                "Uo_data = quantize(\n",
                "    (-0.5) * np.random.random_sample(size=u_shape).astype(np.float32) + 0.25,\n",
                "    quantize_scale,\n",
                ")\n",
                "Uc_data = quantize(\n",
                "    (-0.5) * np.random.random_sample(size=u_shape).astype(np.float32) + 0.25,\n",
                "    quantize_scale,\n",
                ")\n",
                "\n",
                "# Concatenate data as an optimization\n",
                "WUi_data = np.concatenate((Wi_data, Ui_data), axis=0)\n",
                "WUf_data = np.concatenate((Wf_data, Uf_data), axis=0)\n",
                "WUo_data = np.concatenate((Wo_data, Uo_data), axis=0)\n",
                "WUc_data = np.concatenate((Wc_data, Uc_data), axis=0)\n",
                "xhprev_data = np.concatenate((x_data, hprev_data), 1)\n",
                "\n",
                "# Constants\n",
                "log2e = np.ones(shape=v_shape, dtype=np.float32) * np.log2(np.e)\n",
                "ones = np.ones(shape=v_shape, dtype=np.float32)\n",
                "dequantize_vector = np.full(\n",
                "    shape=v_shape, fill_value=dequantize_scale, dtype=np.float32\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## NumPy Baseline\n",
                "\n",
                "We implement a baseline model in NumPy to ensure that our LSTM is functionally correct. By breaking it up into a few functions, the NumPy baseline will match the structure of our Groq API program.\n",
                " - Activation functions for sigm() and tanh(). NumPy has a tanh we can use but we need our own sigmoid.\n",
                " - The dequantize function we defined earlier.\n",
                " - lstmgate_np implements the pattern of matmul -> dequantize -> bias -> activation, which repeats 4 times in an LSTM layer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigm_np(x):\n",
                "    return 1 / (1 + np.exp2(np.log2(np.e) * np.negative(x)))\n",
                "\n",
                "\n",
                "def lstmgate_np(x, hprev, b, W, U, dequantize, act_type):\n",
                "    result = np.matmul(x.astype(np.int32), W.astype(np.int32))\n",
                "    result = result + np.matmul(hprev.astype(np.int32), U.astype(np.int32))\n",
                "\n",
                "    result = dequantize(result, dequantize_scale)\n",
                "\n",
                "    result = result + b\n",
                "\n",
                "    if act_type == \"sigm\":\n",
                "        result = sigm_np(result)\n",
                "    if act_type == \"tanh\":\n",
                "        result = np.tanh(result)\n",
                "\n",
                "    return result\n",
                "\n",
                "\n",
                "def lstm_np(\n",
                "    x, cprev, hprev, bi, bf, bo, bc, Wi, Wf, Wo, Wc, Ui, Uf, Uo, Uc, dequantize\n",
                "):\n",
                "    i_gate = lstmgate_np(x, hprev, bi, Wi, Ui, dequantize, \"sigm\")\n",
                "    f_gate = lstmgate_np(x, hprev, bf, Wf, Uf, dequantize, \"sigm\")\n",
                "    o_gate = lstmgate_np(x, hprev, bo, Wo, Uo, dequantize, \"sigm\")\n",
                "    c_gate = lstmgate_np(x, hprev, bc, Wc, Uc, dequantize, \"tanh\")\n",
                "\n",
                "    cnext = f_gate * cprev + i_gate * c_gate\n",
                "\n",
                "    hnext = np.tanh(cnext) * o_gate\n",
                "\n",
                "    return (cnext, hnext)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Groq API Components\n",
                "We implement a set of composable Groq API components that can be used to build an LSTM that matches our NumPy baseline above. \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### VXM Components Legend\n",
                "We diagram our LSTM VXM components (ALU allocation and chaining) superimposed on the VXM topology below. The topology is not included in these diagrams for readability. \n",
                "\n",
                "VXM diagrams give the ALU number and opcode for each operation.\n",
                "\n",
                "![VXM topology](img/vxm.png)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Dequantize -> Bias\n",
                "\n",
                "We fuse the dequantize and bias functions into a single VXM pass because the LSTM always performs them in sequence. We also write the component so that it can take streaming input, which allows it to be chained after a matmul. \n",
                "\n",
                "To allow 4x concurrency, we find 4 paths through the VXM, two starting in the western hemisphere and 2 starting in the east. These paths must be valid in terms of both VXM placement and stream routing, and cannot conflict with each other. We also specify the layout of our scale and bias constants so they enter the VXM from the same direction as input data. Finally, we have our outputs exit the VXM to the opposite hemisphere from the input, which we will need to take into account when laying out the activation functions.\n",
                "\n",
                "The following diagram shows a single dequantize -> bias VXM kernel:\n",
                "\n",
                "![Dequantize Bias x1](img/dequantize_bias_x1.png)\n",
                "\n",
                "This is the mapping for 4x concurrent DequantizeBias:\n",
                "\n",
                "![Dequantize Bias x4](img/dequantize_bias_x4.png)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "autoscroll": false,
                "ein.hycell": false,
                "ein.tags": "worksheet-0",
                "slideshow": {
                    "slide_type": "-"
                }
            },
            "outputs": [],
            "source": [
                "class DequantizeBias(g.tensor.Component):\n",
                "    def __init__(self, gate_name, bias_data, scale_data, mxm_plane=0, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "\n",
                "        self.gate_name = gate_name\n",
                "        self.bias_data = bias_data\n",
                "        self.scale_data = scale_data\n",
                "        self.mxm_plane = mxm_plane\n",
                "\n",
                "        # Allocate ALUs, streams, and layouts\n",
                "        if mxm_plane == 0:\n",
                "            # Start in NW corner of VXM\n",
                "            alus = [8, 9, 14]\n",
                "            self.output_stream = [g.SG4_E[7]]\n",
                "            self.scale_stream = [g.SG4_E[6]]\n",
                "            self.bias_stream = [g.SG4_E[0]]\n",
                "            self.layout = \"H1(W), -1, S4\"\n",
                "        elif mxm_plane == 1:\n",
                "            # Start in SW corner of VXM\n",
                "            alus = [5, 2, 3]\n",
                "            self.output_stream = [g.SG4_E[3]]\n",
                "            self.scale_stream = [g.SG4_E[1]]\n",
                "            self.bias_stream = [g.SG4_E[2]]\n",
                "            self.layout = \"H1(W), -1, S4\"\n",
                "        elif mxm_plane == 2:\n",
                "            # Start in NE corner of VXM\n",
                "            alus = [10, 13, 12]\n",
                "            self.output_stream = [g.SG4_W[7]]\n",
                "            self.scale_stream = [g.SG4_W[6]]\n",
                "            self.bias_stream = [g.SG4_W[1]]\n",
                "            self.layout = \"H1(E), -1, S4\"\n",
                "        else:  # 3\n",
                "            # Start in SE corner of VXM\n",
                "            alus = [7, 6, 1]\n",
                "            self.output_stream = [g.SG4_W[3]]\n",
                "            self.scale_stream = [g.SG4_W[2]]\n",
                "            self.bias_stream = [g.SG4_W[0]]\n",
                "            self.layout = \"H1(E), -1, S4\"\n",
                "\n",
                "        self.alus = g.tensor.create_alu_request(alus=alus)\n",
                "\n",
                "    def build(self, vec):\n",
                "        # Constants\n",
                "        self.scale_mt = g.from_data(\n",
                "            data=self.scale_data,\n",
                "            name=\"scale_{}\".format(self.gate_name),\n",
                "            layout=self.layout,\n",
                "        )\n",
                "        self.bias_mt = g.from_data(\n",
                "            data=self.bias_data,\n",
                "            name=\"bias_{}\".format(self.gate_name),\n",
                "            layout=self.layout,\n",
                "        )\n",
                "\n",
                "        scale_st = self.scale_mt.read(streams=self.scale_stream)\n",
                "        bias_st = self.bias_mt.read(streams=self.bias_stream)\n",
                "\n",
                "        result = g.cast(\n",
                "            vec,\n",
                "            g.float32,\n",
                "            False,\n",
                "            alus=self.alus[0],\n",
                "            input_streams=self.output_stream,\n",
                "            output_streams=self.output_stream,\n",
                "            time=0,\n",
                "        )\n",
                "        result = g.mul(\n",
                "            result, scale_st, alus=self.alus[1], output_streams=self.output_stream\n",
                "        )\n",
                "        result = g.add(\n",
                "            result, bias_st, alus=self.alus[2], output_streams=self.output_stream\n",
                "        )\n",
                "\n",
                "        return result"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### MvmQuad Component\n",
                "The `MvmQuad` component performs 4 matrix-vector multiplies (MVMs), which stream into 4 dequantize->bias operations in parallel across the 4 MXM planes and 4 VXM rows.\n",
                "\n",
                "The MVMs in a single hemisphere (eg, i and c), are interleaved in time to maximize use of the MEM slices that load the weights. Recall that MVM is a memory bandwidth bound problem, so it is vital to keep the MEM slices working as much as possible.\n",
                "\n",
                " - We map each gate matmul to an independent MXM plane\n",
                " - Weights in the same hemisphere share the same mem slices due to space constraints\n",
                " - Make 4 copies of the input so we can stream in parallel, place each next to its corresponding MXM plane\n",
                " - We start the matmuls at a slight offset, such that the mem slices are almost always busy reading out weights. Remember, this is a SRAM bandwidth-bound problem.\n",
                " - All 4 matmuls stream their results into VXM concurrently, each hitting a different DequantizeBias component in parallel"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MvmQuad(g.tensor.Component):\n",
                "    def __init__(self, WUi, WUf, WUo, WUc, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "\n",
                "        self.WUi_data = WUi\n",
                "        self.WUf_data = WUf\n",
                "        self.WUo_data = WUo\n",
                "        self.WUc_data = WUc\n",
                "        wait_btw_mxm_passes = 40\n",
                "        self.mm_i = nn.MatMul(name=\"WUi_mm\",wait_btw_passes=wait_btw_mxm_passes, planes=[0])\n",
                "        self.mm_c = nn.MatMul(name=\"WUc_mm\",wait_btw_passes=wait_btw_mxm_passes, planes=[1])\n",
                "        self.mm_o = nn.MatMul(name=\"WUo_mm\",wait_btw_passes=wait_btw_mxm_passes, planes=[2])\n",
                "        self.mm_f = nn.MatMul(name=\"WUf_mm\",wait_btw_passes=wait_btw_mxm_passes, planes=[3])\n",
                "\n",
                "    def build(self, xhprev_0_mt, xhprev_1_mt, xhprev_2_mt, xhprev_3_mt, **kwargs):\n",
                "        super().build(**kwargs)\n",
                "        \n",
                "        # Weights: Transpose is necessary because Groq and NumPy expect\n",
                "        # column-major and row-major weights, respectively\n",
                "        self.WUi_mt = g.from_data(\n",
                "            data=self.WUi_data.transpose(),\n",
                "            name=\"WUi_weights\",\n",
                "            layout=\"H1(W), -1, S16(12-39)\",\n",
                "        )\n",
                "\n",
                "        self.WUo_mt = g.from_data(\n",
                "            data=self.WUo_data.transpose(),\n",
                "            name=\"WUo_weights\",\n",
                "            layout=\"H1(E), -1, S16(12-39)\",\n",
                "        )\n",
                "\n",
                "        self.WUc_mt = g.from_data(\n",
                "            data=self.WUc_data.transpose(),\n",
                "            name=\"WUc_weights\",\n",
                "            layout=\"H1(W), -1, S16(12-39)\",\n",
                "        )\n",
                "\n",
                "        self.WUf_mt = g.from_data(\n",
                "            data=self.WUf_data.transpose(),\n",
                "            name=\"WUf_weights\",\n",
                "            layout=\"H1(E), -1, S16(12-39)\",\n",
                "        )\n",
                "        \n",
                "        g.add_mem_constraints(\n",
                "            [self.WUf_mt], \n",
                "            [self.WUo_mt], \n",
                "            g.MemConstraintType.NOT_MUTUALLY_EXCLUSIVE\n",
                "        )\n",
                "        g.add_mem_constraints(\n",
                "            [self.WUi_mt],\n",
                "            [self.WUc_mt],\n",
                "            g.MemConstraintType.NOT_MUTUALLY_EXCLUSIVE,\n",
                "        )\n",
                "        with g.ResourceScope(name=\"mvm_x4\", time=0):\n",
                "            i_product_mt = self.mm_i(xhprev_0_mt, self.WUi_mt, time=0).write(\n",
                "                layout=\"H1(W), -1, S4\", name=\"i_product_mt\"\n",
                "            )\n",
                "            o_product_mt = self.mm_o(xhprev_2_mt, self.WUo_mt, time=0).write(\n",
                "                layout=\"H1(E), -1, S4\", name=\"o_product_mt\"\n",
                "            )\n",
                "            c_product_mt = self.mm_c(xhprev_1_mt, self.WUc_mt, time=20).write(\n",
                "                layout=\"H1(W), -1, S4\", name=\"c_product_mt\"\n",
                "            )\n",
                "            f_product_mt = self.mm_f(xhprev_3_mt, self.WUf_mt, time=20).write(\n",
                "                layout=\"H1(E), -1, S4\", name=\"f_product_mt\"\n",
                "            )\n",
                "\n",
                "        return (i_product_mt, f_product_mt, o_product_mt, c_product_mt)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### MvmQuad Unit Test\n",
                "There's a lot flying around in parallel in `MvmQuad` so we should unit test. We recommend creating a unit test of every significant component of Groq API functionality."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def MvmQuad_unit_test():\n",
                "    g.reset_program_context()\n",
                "\n",
                "    xhprev_0_mt = g.from_data(\n",
                "        data=xhprev_data, name=\"xhprev_cpy0\", layout=\"H1(W), -1, S1(41)\"\n",
                "    )\n",
                "    xhprev_1_mt = g.from_data(\n",
                "        data=xhprev_data, name=\"xhprev_cpy1\", layout=\"H1(W), -1, S1(40)\"\n",
                "    )\n",
                "    xhprev_2_mt = g.from_data(\n",
                "        data=xhprev_data, name=\"xhprev_cpy2\", layout=\"H1(E), -1, S1(41)\"\n",
                "    )\n",
                "    xhprev_3_mt = g.from_data(\n",
                "        data=xhprev_data, name=\"xhprev_cpy3\", layout=\"H1(E), -1, S1(40)\"\n",
                "    )\n",
                "\n",
                "    unit = MvmQuad(WUi_data, WUf_data, WUo_data, WUc_data,)\n",
                "\n",
                "    (i_mt, f_mt, o_mt, c_mt) = unit(\n",
                "        xhprev_0_mt, xhprev_1_mt, xhprev_2_mt, xhprev_3_mt\n",
                "    )\n",
                "\n",
                "    print(\"Compiling...\")\n",
                "\n",
                "    # Compile and run\n",
                "    iop_file = g.compile(\n",
                "        base_name=\"mvmquad_test\", result_tensor=[i_mt, f_mt, o_mt, c_mt]\n",
                "    )\n",
                "    gate_program = tsp.create_tsp_runner(iop_file)\n",
                "\n",
                "    result = gate_program()\n",
                "\n",
                "    g.write_visualizer_data(\"quad_mvm\")\n",
                "\n",
                "    # Oracle\n",
                "    oracle_i = np.matmul(xhprev_data.astype(np.int32), WUi_data.astype(np.int32))\n",
                "    oracle_o = np.matmul(xhprev_data.astype(np.int32), WUo_data.astype(np.int32))\n",
                "    oracle_c = np.matmul(xhprev_data.astype(np.int32), WUc_data.astype(np.int32))\n",
                "    oracle_f = np.matmul(xhprev_data.astype(np.int32), WUf_data.astype(np.int32))\n",
                "\n",
                "    np.testing.assert_allclose(result[\"i_product_mt\"], oracle_i, atol=0.01)\n",
                "    np.testing.assert_allclose(result[\"o_product_mt\"], oracle_o, atol=0.01)\n",
                "    np.testing.assert_allclose(result[\"c_product_mt\"], oracle_c, atol=0.01)\n",
                "    np.testing.assert_allclose(result[\"f_product_mt\"], oracle_f, atol=0.01)\n",
                "\n",
                "    print(\"MvmQuad unit test success\")\n",
                "\n",
                "\n",
                "MvmQuad_unit_test()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Sigmoid (sigm)\n",
                "Our sigmoid function is based on Groq API's nn.sigmoid, however we wrote our own to get the flow of data we need for our top-level LSTM mapping. Specifically, we want data to enter the VXM on one side and exit on the opposite side, versus the nn.sigmoid, which has data exit the VXM on the same side it entered.\n",
                "\n",
                "Our numerics are the same as our NumPy for the sigmoid above with one exception: GroqChip instructions do not include a division (`d = y/x`) or reciprocal instruction (`1/x`), so we implement division as `d = y * rsqrt(x)^2`.\n",
                "\n",
                "We enable 2x concurrency by finding one mapping that stays in the top 2 rows of the VXM and another that stays in the bottom 2 rows.\n",
                "\n",
                "A single sigmoid looks like this:\n",
                "\n",
                "![Sigmoid_x1](img/sigm_x1.png)\n",
                "\n",
                "The two concurrent sigmoids together look like this:\n",
                "\n",
                "![Sigmoid_x2](img/sigm_x2.png)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Sigm(g.tensor.Component):\n",
                "    def __init__(self, use_upper=True, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "\n",
                "        # Flow = neg (small) - mul (small) - exp (large) - add (small) - rsqrt (large) - mul (small)\n",
                "        if use_upper:\n",
                "            alus = [12, 13, 8, 9, 10, 11]\n",
                "            self.compute_streams = [\n",
                "                g.SG4_E[7],\n",
                "                g.SG4_E[7],\n",
                "                g.SG4_W[7],\n",
                "                g.SG4_E[5],\n",
                "                g.SG4_E[5],\n",
                "                g.SG4_E[5],\n",
                "                g.SG4_E[5],\n",
                "            ]\n",
                "            self.log2e_stream = g.SG4_E[6]\n",
                "            self.ones_stream = g.SG4_E[4]\n",
                "            self.layout = \"H1(W), -1, S4\"\n",
                "        else:\n",
                "            alus = [3, 2, 7, 6, 5, 4]\n",
                "            self.compute_streams = [\n",
                "                g.SG4_W[1],\n",
                "                g.SG4_W[1],\n",
                "                g.SG4_E[2],\n",
                "                g.SG4_W[2],\n",
                "                g.SG4_W[2],\n",
                "                g.SG4_W[2],\n",
                "                g.SG4_W[2],\n",
                "            ]\n",
                "            self.log2e_stream = g.SG4_W[0]\n",
                "            self.ones_stream = g.SG4_W[3]\n",
                "            self.layout = \"H1(E), -1, S4\"\n",
                "\n",
                "        self.alus = g.tensor.create_alu_request(alus=alus)\n",
                "\n",
                "    def build(self, vec):\n",
                "        # Constants\n",
                "        log2e_mt = g.from_data(data=log2e, name=\"log2e\", layout=self.layout)\n",
                "        ones_mt = g.from_data(data=ones, name=\"ones\", layout=self.layout)\n",
                "\n",
                "        log2e_st = log2e_mt.read(streams=self.log2e_stream)\n",
                "        ones_st = ones_mt.read(streams=self.ones_stream)\n",
                "\n",
                "        vec_st = vec.read(streams=self.compute_streams[0])\n",
                "\n",
                "        result_st = g.neg(\n",
                "            vec_st, alus=self.alus[0], output_streams=self.compute_streams[1], time=0,\n",
                "        )\n",
                "        result_st = g.mul(\n",
                "            result_st,\n",
                "            log2e_st,\n",
                "            alus=self.alus[1],\n",
                "            output_streams=self.compute_streams[2],\n",
                "        )\n",
                "        result_st = g.exp2(\n",
                "            result_st, alus=self.alus[2], output_streams=self.compute_streams[3]\n",
                "        )\n",
                "        result_st = g.add(\n",
                "            result_st,\n",
                "            ones_st,\n",
                "            alus=self.alus[3],\n",
                "            output_streams=self.compute_streams[4],\n",
                "        )\n",
                "        result_st = g.rsqrt(\n",
                "            result_st, alus=self.alus[4], output_streams=self.compute_streams[5]\n",
                "        )\n",
                "        result_st = g.mul(\n",
                "            result_st,\n",
                "            result_st,\n",
                "            alus=self.alus[5],\n",
                "            output_streams=self.compute_streams[6],\n",
                "        )\n",
                "\n",
                "        return result_st"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Hyperbolic Tangent (tanh)\n",
                "Our tanh function uses 5 VXM ALUs to perform the entire function in a single chain. This function leverages the VXM's special `tanh` instruction, which requires the data to be scaled by a constant `pre-tanh`, cast to `int16`, then the `tanh` instruction, cast back to `float32`, and finally scaled by another constant `post-tanh`.\n",
                "\n",
                "Our route through the VXM satisfies two use cases:\n",
                "1. Concurrency with a `sigmoid` occupying the upper two rows of VXM\n",
                "2. Composability with the other ops for `lstm reduction` \n",
                "\n",
                "Note that the code allows Groq API to infer the direction of the input stream, and we let the user set the direction of the output stream. This enables the flexibility for the two use cases above.\n",
                "\n",
                "Tanh by itself (LSTM doesn't do this, but it's the simplest view of the tanh):\n",
                "\n",
                "![Tanh](img/tanh.png)\n",
                "\n",
                "Tanh concurrency with sigmoid:\n",
                "\n",
                "![Tanh and Sigmoid](img/tanh_and_sigmoid.png)\n",
                "\n",
                "See the LSTM Reduction section below to see how tanh is incorporated as a subcomponent.\n",
                "                           "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Tanh(g.tensor.Component):\n",
                "    def __init__(self, output_stream, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "\n",
                "        self.output_stream = output_stream\n",
                "\n",
                "    def build(self, vec):\n",
                "        if vec.is_memory_tensor():\n",
                "            start_time = 0\n",
                "            vec_st = vec.read(streams=g.SG4_W[5])\n",
                "        else:\n",
                "            # vec is a stream, and we should inherit time from the stream\n",
                "            start_time = None\n",
                "            vec_st = vec\n",
                "\n",
                "        pre_tanh_mt = g.from_data(\n",
                "            np.ones((1, v_shape[1]), dtype=np.float32) * 5461.17,\n",
                "            name=\"pre_tanh_mt\",\n",
                "            layout=\"H1(E),-1,S4\",\n",
                "        )\n",
                "        post_tanh_mt = g.from_data(\n",
                "            np.ones((1, v_shape[1]), dtype=np.float32) * 3.05185e-05,\n",
                "            name=\"post_tanh_mt\",\n",
                "            layout=\"H1(W),-1,S4\",\n",
                "        )\n",
                "\n",
                "        pre_tanh_st = pre_tanh_mt.read(streams=g.SG4_W[4])\n",
                "        post_tanh_st = post_tanh_mt.read(streams=g.SG4_E[3])\n",
                "\n",
                "        tmp_st = vec_st.mul(\n",
                "            pre_tanh_st, alus=[6], output_streams=[g.SG4_E[3]], time=start_time\n",
                "        )\n",
                "        tmp_st = tmp_st.cast(\n",
                "            dtype=g.int16, alus=[7], output_streams=[g.SG4_W[3]], fp16_inf=False\n",
                "        )\n",
                "        tmp_st = tmp_st.tanh(alus=[2], output_streams=[g.SG4_W[2]])\n",
                "        tmp_st = tmp_st.cast(\n",
                "            dtype=g.float32, alus=[5], output_streams=[g.SG4_W[2]], fp16_inf=False\n",
                "        )\n",
                "        tmp_st = tmp_st.mul(\n",
                "            post_tanh_st, alus=[4], output_streams=[self.output_stream]\n",
                "        )\n",
                "\n",
                "        return tmp_st"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### LSTM Reduction\n",
                "We pack all of the pointwise ops after the LSTM gates into a single component, which executes all ops in a single VXM chain:\n",
                "\n",
                "    cnext = f_gate * cprev + i_gate * c_gate\n",
                "    hnext = tanh_np(cnext) * o_gate\n",
                "\n",
                "The LSTM Reduction VXM mapping looks like this:\n",
                "\n",
                "![LSTM Reduction](img/lstm_reduction.png)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LstmReduction(g.tensor.Component):\n",
                "    def __init__(self, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "        self.tanh = Tanh(g.SG4_E[3])\n",
                "\n",
                "    def build(self, fgate_mt, cprev_mt, igate_mt, cgate_mt, ogate_mt, **kwargs):\n",
                "        super().build(**kwargs)\n",
                "        \n",
                "        cgate_st = cgate_mt.read(streams=g.SG4_E[7])\n",
                "        igate_st = igate_mt.read(streams=g.SG4_E[6])\n",
                "        fgate_st = fgate_mt.read(streams=g.SG4_W[6])\n",
                "        cprev_st = cprev_mt.read(streams=g.SG4_W[7])\n",
                "        ogate_st = ogate_mt.read(streams=g.SG4_W[0])\n",
                "\n",
                "        ic_st = igate_st.mul(cgate_st, alus=[12], output_streams=[g.SG4_E[6]], time=0)\n",
                "        fcprev_st = fgate_st.mul(\n",
                "            cprev_st, alus=[15], output_streams=[g.SG4_W[6]]\n",
                "        )\n",
                "        cnext_st = fcprev_st.add(\n",
                "            ic_st, alus=[9], output_streams=[g.SG4_E[4], g.SG4_E[5]]\n",
                "        )\n",
                "        cnext_mt = cnext_st.write(\n",
                "            streams=g.SG4_E[4], name=\"cnext_out\", layout=\"H1(E), -1, S4\"\n",
                "        )\n",
                "\n",
                "        tmp_st = self.tanh(cnext_st)\n",
                "        hnext_mt = tmp_st.mul(ogate_st, alus=[1], output_streams=[g.SG4_W[1]]).write(\n",
                "            name=\"hnext_out\", layout=\"H1(W), -1, S4\"\n",
                "        )\n",
                "\n",
                "        return (cnext_mt, hnext_mt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def LstmReduction_unit_test():\n",
                "    g.reset_program_context()\n",
                "\n",
                "    fgate_data = np.random.rand(v_shape[0], v_shape[1]).astype(np.float32)\n",
                "    cprev_data = np.random.rand(v_shape[0], v_shape[1]).astype(np.float32)\n",
                "    igate_data = np.random.rand(v_shape[0], v_shape[1]).astype(np.float32)\n",
                "    cgate_data = np.random.rand(v_shape[0], v_shape[1]).astype(np.float32)\n",
                "    ogate_data = np.random.rand(v_shape[0], v_shape[1]).astype(np.float32)\n",
                "\n",
                "    fgate_mt = g.from_data(fgate_data, name=\"fgate\", layout=\"H1(E), -1, S4\")\n",
                "    cprev_mt = g.from_data(cprev_data, name=\"cprev\", layout=\"H1(E), -1, S4\")\n",
                "    igate_mt = g.from_data(igate_data, name=\"igate\", layout=\"H1(W), -1, S4\")\n",
                "    cgate_mt = g.from_data(cgate_data, name=\"cgate\", layout=\"H1(W), -1, S4\")\n",
                "    ogate_mt = g.from_data(ogate_data, name=\"ogate\", layout=\"H1(E), -1, S4\")\n",
                "\n",
                "    unit = LstmReduction()\n",
                "\n",
                "    (cnext_mt, hnext_mt) = unit(fgate_mt, cprev_mt, igate_mt, cgate_mt, ogate_mt)\n",
                "\n",
                "    print(\"Compiling...\")\n",
                "\n",
                "    # Compile and run\n",
                "    iop_file = g.compile(\n",
                "        base_name=\"lstmreduction_test\", result_tensor=[cnext_mt, hnext_mt]\n",
                "    )\n",
                "    gate_program = tsp.create_tsp_runner(iop_file)\n",
                "\n",
                "    result = gate_program()\n",
                "\n",
                "    g.write_visualizer_data(\"lstm_reduction\")\n",
                "\n",
                "    # Oracle\n",
                "    def oracle_np(fgate, cprev, igate, cgate, ogate):\n",
                "        cnext = (fgate * cprev) + (igate * cgate)\n",
                "        hnext = np.tanh(cnext) * ogate\n",
                "        return (cnext, hnext)\n",
                "\n",
                "    (cnext_np, hnext_np) = oracle_np(\n",
                "        fgate_data, cprev_data, igate_data, cgate_data, ogate_data\n",
                "    )\n",
                "\n",
                "    np.testing.assert_allclose(result[\"cnext_out\"], cnext_np, atol=0.01)\n",
                "    np.testing.assert_allclose(result[\"hnext_out\"], hnext_np, atol=0.01)\n",
                "\n",
                "    print(\"LstmReduction unit test success\")\n",
                "\n",
                "\n",
                "LstmReduction_unit_test()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Copying the Input\n",
                "We create a quick component to take the input data (`xhprev_mt`, the concatenation of x and hprev) and copy it 4 times. We do this to produce an independent copy for each MXM plane to use concurrently.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CopyQuad(g.tensor.Component):\n",
                "    def __init__(self, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "\n",
                "    def build(self, input_mt, **kwargs):\n",
                "        super().build(**kwargs)\n",
                "\n",
                "        with g.ResourceScope(\n",
                "            name=\"input_copying_W\", is_buffered=True, time=0\n",
                "        ) as input_copying_W:\n",
                "            input_st = input_mt.read(streams=g.SG1_W, time=0)\n",
                "            copy_0W_mt = input_st.write(name=\"copy_0W_mt\", layout=\"H1(W), -1, S1(41)\")\n",
                "            copy_1W_mt = input_st.write(name=\"copy_1W_mt\", layout=\"H1(W), -1, S1(40)\")\n",
                "\n",
                "        with g.ResourceScope(\n",
                "            name=\"input_copying_E\",\n",
                "            is_buffered=True,\n",
                "            predecessors=[input_copying_W],\n",
                "            time=None,\n",
                "        ) as input_copying_E:\n",
                "            input_st = input_mt.read(streams=g.SG1_E, time=0)\n",
                "            copy_0E_mt = input_st.write(name=\"copy_0E_mt\", layout=\"H1(E), -1, S1(41)\")\n",
                "            copy_1E_mt = input_st.write(name=\"copy_1E_mt\", layout=\"H1(E), -1, S1(40)\")\n",
                "\n",
                "        return (copy_0W_mt, copy_1W_mt, copy_0E_mt, copy_1E_mt)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Activations Component\n",
                "We group all of the activation functions together to take advantage of the memory allocation features of buffered components. Specifically, we want to make sure that all of the inputs to LstmReduction are in mutually exclusive memory slices. This is accomplished by placing the producers of those tensors (`i_gate`, `f_gate`, `c_gate`, and `o_gate`) into the same component.\n",
                "\n",
                "We'll do the same thing for the 4x dequantize bias for the sake of consistency in our programming style."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class QuadActivations(g.tensor.Component):\n",
                "    def __init__(self, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "\n",
                "        self.i_activation = Sigm(name=\"i_activation\", use_upper=False)\n",
                "        self.c_activation = Tanh(name=\"c_activation\", output_stream=g.SG4_W[2])\n",
                "\n",
                "        self.f_activation = Sigm(name=\"f_activation\", use_upper=True)\n",
                "        self.o_activation = Sigm(name=\"o_activation\", use_upper=True)\n",
                "\n",
                "    def build(self, i_partial_mt, f_partial_mt, c_partial_mt, o_partial_mt, **kwargs):\n",
                "        super().build(**kwargs)\n",
                "\n",
                "        with g.ResourceScope(\n",
                "            name=\"i_o_activations\", is_buffered=True, time=0,\n",
                "        ) as i_o_activations:\n",
                "            i_st = self.i_activation(i_partial_mt)\n",
                "            igate_mt = i_st.write(name=\"igate_mt\", layout=\"H1(W), -1, S4\")\n",
                "\n",
                "            o_st = self.o_activation(o_partial_mt)\n",
                "            ogate_mt = o_st.write(name=\"ogate_mt\", layout=\"H1(E), -1, S4\")\n",
                "\n",
                "        with g.ResourceScope(\n",
                "            name=\"f_c_activations\",\n",
                "            is_buffered=True,\n",
                "            time=None,\n",
                "            predecessors=[i_o_activations],\n",
                "        ) as f_c_activations:\n",
                "            c_st = self.c_activation(c_partial_mt)\n",
                "            cgate_mt = c_st.write(name=\"cgate_mt\", layout=\"H1(W), -1, S4\")\n",
                "\n",
                "            f_st = self.f_activation(f_partial_mt)\n",
                "            fgate_mt = f_st.write(name=\"fgate_mt\", layout=\"H1(E), -1, S4\")\n",
                "\n",
                "        return (igate_mt, fgate_mt, cgate_mt, ogate_mt)\n",
                "\n",
                "\n",
                "class QuadDequantizeBias(g.tensor.Component):\n",
                "    def __init__(self, bi, bf, bo, bc, dequantize, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "\n",
                "        self.dequantize_bias_i = DequantizeBias(\"i\", bi, dequantize, mxm_plane=0)\n",
                "        self.dequantize_bias_c = DequantizeBias(\"c\", bc, dequantize, mxm_plane=1)\n",
                "        self.dequantize_bias_o = DequantizeBias(\"o\", bo, dequantize, mxm_plane=2)\n",
                "        self.dequantize_bias_f = DequantizeBias(\"f\", bf, dequantize, mxm_plane=3)\n",
                "\n",
                "    def build(self, i_product_mt, f_product_mt, o_product_mt, c_product_mt, **kwargs):\n",
                "        super().build(**kwargs)\n",
                "\n",
                "        i_partial_mt = self.dequantize_bias_i(i_product_mt).write(\n",
                "            name=\"i_partial\", layout=\"H1(E), -1, S4\"\n",
                "        )\n",
                "        o_partial_mt = self.dequantize_bias_o(o_product_mt).write(\n",
                "            name=\"o_partial\", layout=\"H1(W), -1, S4\"\n",
                "        )\n",
                "        c_partial_mt = self.dequantize_bias_c(c_product_mt).write(\n",
                "            name=\"c_partial\", layout=\"H1(E), -1, S4\"\n",
                "        )\n",
                "        f_partial_mt = self.dequantize_bias_f(f_product_mt).write(\n",
                "            name=\"f_partial\", layout=\"H1(W), -1, S4\"\n",
                "        )\n",
                "\n",
                "        return (i_partial_mt, f_partial_mt, c_partial_mt, o_partial_mt)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Top-Level LSTM Component\n",
                "Now we build the optimized LSTM component out of our existing components to match the LSTM optimization plan from the top of the notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Lstm(g.tensor.Component):\n",
                "    def __init__(self, bi, bf, bo, bc, WUi, WUf, WUo, WUc, dequantize, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "\n",
                "        self.copy_x4 = CopyQuad(name=\"copy_x4\", is_buffered=True)\n",
                "        self.mvm_x4 = MvmQuad(\n",
                "            WUi,\n",
                "            WUf,\n",
                "            WUo,\n",
                "            WUc,\n",
                "            name=\"mvm_x4\",\n",
                "            is_buffered=True,\n",
                "            is_resource_scope=True,\n",
                "        )\n",
                "        self.dequantize_bias_x4 = QuadDequantizeBias(\n",
                "            bi,\n",
                "            bf,\n",
                "            bo,\n",
                "            bc,\n",
                "            dequantize,\n",
                "            name=\"dequantize_bias_x4\",\n",
                "            is_buffered=True,\n",
                "            is_resource_scope=True,\n",
                "        )\n",
                "        self.activations_x4 = QuadActivations(name=\"activations_x4\", is_buffered=True)\n",
                "        self.reduction = LstmReduction(\n",
                "            name=\"reduction\", is_buffered=True, is_resource_scope=True\n",
                "        )\n",
                "\n",
                "    def build(self, xhprev_mt, cprev_mt):\n",
                "\n",
                "        # Copy the input 4 times\n",
                "        (xhprev_0_mt, xhprev_1_mt, xhprev_2_mt, xhprev_3_mt) = self.copy_x4(xhprev_mt, time=0)\n",
                "\n",
                "        # Perform 4x MVMs in parallel\n",
                "        (i_product_mt, f_product_mt, o_product_mt, c_product_mt) = self.mvm_x4(\n",
                "            xhprev_0_mt, xhprev_1_mt, xhprev_2_mt, xhprev_3_mt, predecessors=[self.copy_x4]\n",
                "        )\n",
                "\n",
                "        # Perform 4x dequantize -> bias operations\n",
                "        (\n",
                "            i_partial_mt,\n",
                "            f_partial_mt,\n",
                "            c_partial_mt,\n",
                "            o_partial_mt,\n",
                "        ) = self.dequantize_bias_x4(\n",
                "            i_product_mt,\n",
                "            f_product_mt,\n",
                "            o_product_mt,\n",
                "            c_product_mt,\n",
                "            predecessors=[self.mvm_x4],\n",
                "        )\n",
                "\n",
                "        # Run the activation function on each of our 4 tensors\n",
                "        (igate_mt, fgate_mt, cgate_mt, ogate_mt) = self.activations_x4(\n",
                "            i_partial_mt,\n",
                "            f_partial_mt,\n",
                "            c_partial_mt,\n",
                "            o_partial_mt,\n",
                "            predecessors=[self.dequantize_bias_x4],\n",
                "        )\n",
                "\n",
                "        # Final reduction operations to get c next and h next\n",
                "        (cnext_mt, hnext_mt) = self.reduction(\n",
                "            fgate_mt,\n",
                "            cprev_mt,\n",
                "            igate_mt,\n",
                "            cgate_mt,\n",
                "            ogate_mt,\n",
                "            predecessors=[self.activations_x4],\n",
                "        )\n",
                "\n",
                "        return (cnext_mt, hnext_mt)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### LSTM Unit Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "def Lstm_unit_test():\n",
                "    g.reset_program_context()\n",
                "    \n",
                "    xhprev_mt = g.from_data(\n",
                "        data=xhprev_data, name=\"xhprev_cpy0\", layout=\"H1(W), -1, S1(0)\"\n",
                "    )\n",
                "\n",
                "    cprev_mt = g.from_data(\n",
                "        data=cprev_data, name=\"cprev_mt\", layout=\"H1(E), -1, S4\"\n",
                "    )\n",
                "\n",
                "    model = Lstm(\n",
                "        bi_data,\n",
                "        bf_data,\n",
                "        bo_data,\n",
                "        bc_data,\n",
                "        WUi_data,\n",
                "        WUf_data,\n",
                "        WUo_data,\n",
                "        WUc_data,\n",
                "        dequantize_vector,\n",
                "    )\n",
                "\n",
                "    (cnext_mt, hnext_mt) = model(xhprev_mt, cprev_mt)\n",
                "\n",
                "    print(\"Compiling...\")\n",
                "\n",
                "    # Compile and run\n",
                "    iop_file = g.compile(\n",
                "        base_name=\"lstm_test\", result_tensor=[cnext_mt, hnext_mt]\n",
                "    )\n",
                "    gate_program = tsp.create_tsp_runner(iop_file)\n",
                "\n",
                "    result = gate_program()\n",
                "\n",
                "    g.write_visualizer_data(\"lstm\")\n",
                "\n",
                "    (cnext_np, hnext_np) = lstm_np(\n",
                "        x_data,\n",
                "        cprev_data,\n",
                "        hprev_data,\n",
                "        bi_data,\n",
                "        bf_data,\n",
                "        bo_data,\n",
                "        bc_data,\n",
                "        Wi_data,\n",
                "        Wf_data,\n",
                "        Wo_data,\n",
                "        Wc_data,\n",
                "        Ui_data,\n",
                "        Uf_data,\n",
                "        Uo_data,\n",
                "        Uc_data,\n",
                "        dequantize,\n",
                "    )\n",
                "\n",
                "    np.testing.assert_allclose(result[\"cnext_out\"], cnext_np, atol=0.01)\n",
                "    np.testing.assert_allclose(result[\"hnext_out\"], hnext_np, atol=0.01)\n",
                "\n",
                "    print(\"Optimized LSTM unit test success\")\n",
                "\n",
                "\n",
                "Lstm_unit_test()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def calcLstmPerformance(input_size, hidden_size, batch, cycles):\n",
                "    alan_clock_rate = 900000000\n",
                "\n",
                "    # Calculate ops as:\n",
                "    #  - Consider MXM ops only\n",
                "    #  - MVM of size [1 x N] * [N x M] has M*N multiply-accumulates (MACs)\n",
                "    #  - 1 add and 1 multiply op per MAC\n",
                "    #  - Matrix size is (input_size + hidden_size) * hidden_size\n",
                "    ops = batch * 2 * 4 * (input_size + hidden_size) * hidden_size\n",
                "    seconds = cycles / alan_clock_rate\n",
                "    microseconds = seconds * 1000000\n",
                "    tops = (ops / seconds) / (1000000000000)\n",
                "\n",
                "    print(\"----- Performance -----\")\n",
                "\n",
                "    print(\"batch                    =\", batch)\n",
                "    print(\"input size               =\", input_size)\n",
                "    print(\"hidden size              =\", hidden_size)\n",
                "    print(\"ops                      =\", ops)\n",
                "    print(\"latency (cycles)         =\", cycles)\n",
                "    print(\"latency (microseconds)   = {0:.2f}\".format(microseconds))\n",
                "    print(\"tops                     = {0:.2f}\".format(tops))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Get the performance\n",
                "1. Run the GroqView command `groqview lstm/visdata.json`\n",
                "1. Run `calcLstmPerformance()` against the GroqView cycle count"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "calcLstmPerformance(512, 512, 1, 670)\n",
                "calcLstmPerformance(512, 512, 3, 718)\n",
                "calcLstmPerformance(512, 512, 6, 790)\n",
                "calcLstmPerformance(512, 512, 8, 838)\n",
                "calcLstmPerformance(512, 512, 32, 1413)\n",
                "\n",
                "calcLstmPerformance(1024, 1024, 1, 1958)\n",
                "calcLstmPerformance(1024, 1024, 3, 2086)\n",
                "calcLstmPerformance(1024, 1024, 6, 2278)\n",
                "calcLstmPerformance(1024, 1024, 8, 2406)\n",
                "calcLstmPerformance(1024, 1024, 32, 3941)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## State of the Art (SOTA) Comparison\n",
                "[The most recent paper on LSTM acceleration is a collaboration between FPGA royalty from Intel and top universities published in late 2020.](https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/a1153843-beyond-peak-performance-white-paper.pdf)\n",
                "\n",
                "FPGAs are a strong target for LSTM acceleration thanks to:\n",
                " - Many small distributed SRAMs that provide high memory bandwidth to MVMs\n",
                " - Flexibility to specify deeply pipelined datapaths for pointwise vector ops\n",
                "\n",
                "The paper above targets the Intel Stratix 10 NX FPGA, which Intel's Stratix 10 MX (HBM) FPGA modified with special block floating point matmul digital signal processing units. The device has a theoretical peak 120 TOPS at 500 MHz at int8 / block-float-16 precision. \n",
                "\n",
                "The authors map a clone of the Microsoft Brainwave RNN architecture to the NX and compare against Nvidia's V100 and T4 GPUs. See below for a comparison of results across different LSTM sizes and batch sizes.\n",
                "\n",
                "Copy of Intel's results, for convenience: \n",
                "\n",
                "![Intel LSTM benchmarks](img/intel-lstm-results.png)\n",
                "\n",
                "Intel's results for LSTM-512, plus GroqChip 1 results:\n",
                "\n",
                "![Groq LSTM-512 benchmarks](img/lstm-512.png)\n",
                "\n",
                "Intel's results for LSTM-1024, plus GroqChip 1 results:\n",
                "\n",
                "![Groq LSTM-1024 benchmarks](img/lstm-1024.png)"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "126097c9c1df48e3bcdeb1bbd93cb6724fd6717eec51a46fb371cdea1ef1837b"
        },
        "kernelspec": {
            "display_name": "Python 3.6.13 64-bit ('default')",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        },
        "name": "helloworld.ipynb"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}