{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq API - Matrix Multiplication Tutorial\n",
    "\n",
    "In this example, we'll introduce how to perform a matrix multiplication in the MXM module of the GroqChip. In the Adding Tensors and the Buffered Scopes tutorials, we created our own components (Add and Mul), this time we'll use the premade matmul component included with the Groq API Neural Net Library (NN). \n",
    "\n",
    "By the end of this tutorial, you should feel comfortable with the following concepts:\n",
    "* Matrix Multiplication on Groq hardware\n",
    "* MXM: Matrix Execution Module\n",
    "* Groq API Neural Net Library\n",
    "\n",
    "It is expected that you have finished reading the Intro to Matrix Multiplication section of the Groq API Tutorial Guide prior to going through this tutorial. \n",
    "\n",
    "## Build a program and Compile with Groq API\n",
    "Begin by importing the following packages. Note that for this example, in addition to the Groq API, we're also importing the Neural Net library from the Groq API as 'nn'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import groq.api as g\n",
    "import groq.api.nn as nn\n",
    "from groq.runner import tsp\n",
    "import numpy as np\n",
    "print(\"Python packages imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two input tensors as placeholders for the data we're going to multiply. The Matrix Multiply in the Neural Net library expects two Rank-2 tensors and supports the following data types: int8 & float16, as well as a special case for mixed FLOAT16/FLOAT32 (See API Reference Guide for more). The API implicitly transposes the 2nd tensor before performing the matmul operation. As well, it is required that the inner dimension of both memory tensors are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = g.input_tensor(shape=(120, 120), dtype=g.float16, name=\"matrix1\", layout=\"H1(W), -1, S2\")\n",
    "matrix2 = g.input_tensor(shape=(120, 120), dtype=g.float16, name=\"matrix2\", layout=\"H1(W), -1, S16(4-38)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see in the above that we include a layout for the input tensors--while not required for this simple example, this is a best practice guideline and should be included. The details of memory layouts will be explained in the Multi Matmul Tutorial but for float16, the following layout is recommended. See the API Reference Guide section on `nn.matmul()` for guidance. \n",
    "\n",
    "The following instantiates the Neural Net matmul component inside your top level component. It is recommended to provide a name to your matmul operation to help with any future debug needs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopLevel(g.Component):  # Create our top level component\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mm = nn.MatMul(name=\"MyMatMul\", buffer_output=True)     #Matmul: using the nn.MatMul() component.\n",
    "\n",
    "    def build(self, mat1_mt, mat2_mt, time=0):   #Provide input matrices and a default time\n",
    "        with g.ResourceScope(name=\"mmscope\", time=0) as mmscope:\n",
    "            result_mt = self.mm(mat1_mt, mat2_mt, time=0)\n",
    "            result_mt.name = \"mm_result\"\n",
    "            result_mt.layout = \"H1(W), -1, S4\" #recommended layout for the matmul result (float32)\n",
    "        return result_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = TopLevel()    # instantiate the top level component\n",
    "result = top(matrix1, matrix2, time=0)    # call into the instance of the top level, providing your inputs and time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've instantiated and built the MatMul component, we can compile our program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iop_file = g.compile(base_name=\"matmul_tutorial\", result_tensor=result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroqView\n",
    "GroqView can be used to view the instructions of your program in the GroqChip. Note: it is expected that you are familiar with the GroqView tool (See \"GroqView User Guide\") for this section of this tutorial. You may skip viewing the program in GroqView and move to the \"Prepare Data for Program\" section.\n",
    "\n",
    "Using the following command, we can create a .json file that can be used to view the program in hardware. This will show:\n",
    "* what instructions occur\n",
    "* where on the chip they take place, as well as \n",
    "* when in time (cycles) each instruction occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.write_visualizer_data(\"matmul\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To launch GroqView, uncomment and run the following command. Remember, you still need to create a tunnel to the server running the GroqView tool to load in another window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!groqview matmul/visdata.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> before proceeding to the next section, you'll want to stop the above cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_data = np.random.rand(120, 120).astype(np.float16)\n",
    "t2_data = np.random.rand(120, 120).astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on Hardware\n",
    "Program the GroqChip with the binary file of the Matrix Multiply program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program = tsp.create_tsp_runner(iop_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the input data to the GroqChip which will return the results of the matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = program(matrix1=t1_data, matrix2=t2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Results\n",
    "Note that the oracle value is float32 because the output of the MXM matrix multiply is float32 for two float16 inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle = np.matmul(t1_data, t2_data.transpose(), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matrix Multiplication for input tensors of size {} x {}.  Results are: \".format(t1_data.shape, t2_data.shape))\n",
    "print(np.allclose(oracle, result['mm_result'], rtol=1e-1, atol=1e-1, equal_nan=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Back Computations\n",
    "The GroqChip is still programmed with the matmul program so we can continue to provide input data and it will return the results of the matmul. Now let's look at how we can perform calls to the same program repeatedly with different input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Matrix Multiply {i}\")\n",
    "    t1_data = np.random.rand(120, 120).astype(np.float16)\n",
    "    t2_data = np.random.rand(120, 120).astype(np.float16)\n",
    "    result = program(matrix1=t1_data, matrix2=t2_data)\n",
    "    oracle = np.matmul(t1_data, t2_data.transpose(), dtype=np.float32)\n",
    "    print(\"For input tensors of size {} x {}. Results are: \".format(t1_data.shape, t2_data.shape))\n",
    "    print(np.allclose(oracle, result['mm_result'], rtol=1e-1, atol=1e-1, equal_nan=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional\n",
    "Try different sized matmuls and see what happens in the hardware using the GroqView tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
