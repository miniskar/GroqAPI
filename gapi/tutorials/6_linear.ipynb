{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq API - Linear Tutorial\n",
    "\n",
    "The following tutorial will implement a Linear Layer that includes:\n",
    "    Matmul -> Bias Add -> ReLu\n",
    "\n",
    "By the end of this tutorial, you should feel comfortable with the following concepts:\n",
    "* VXM Chaining\n",
    "\n",
    "It is expected that you have finished reading the Linear Layer section of the Groq API Tutorial Guide prior to going through this tutorial. \n",
    "\n",
    "## Build Your Program\n",
    "Begin by importing the following packages. Since we'll be using the matmul and relu component from the Neural Net library, we import groq.api.nn as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import groq.api as g\n",
    "import groq.api.nn as nn\n",
    "import numpy as np\n",
    "from groq.runner import tsp\n",
    "\n",
    "print(\"Python packages imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Program\n",
    "First, we'll setup our input data. We've selected a size of 320 so that we can show a full MXM matrix multiplication. To fully exercise our linear layer, we will need data with both positive and negative numbers, so we start with a random sample between 0 and 1 and subtract -0.5 to get a range between -0.5 and 0.5. This is so that the ReLU has an effect as it sets negative values to zero and passes anything greater than 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 320\n",
    "v_shape = (1, size)       # Vector shape for the bias add\n",
    "mat_shape = (size, size)  # Size for Input data and weights\n",
    "\n",
    "input_data = np.random.random_sample(size=mat_shape).astype(np.float16) - 0.5\n",
    "\n",
    "bias_data = np.random.random_sample(size=v_shape).astype(np.float32)  # Bias is FP32 as the matmul from the MXM returns results in FP32\n",
    "weights_data = np.random.random_sample(size=mat_shape).astype(np.float16) - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create an input data placeholder for the activations of the matmul. We use the recommended layout for the matmul activations for FP16. This is discussed in the Multi-Matmul Tutorial or can be referenced in the Groq API Reference Guide for nn.matmul(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mt = g.input_tensor(\n",
    "    shape=mat_shape,\n",
    "    name=\"input_matrix\",            # always name when possible\n",
    "    layout=\"H1(W), -1, S2(4-38)\",   # layout to match the activation matrix. \n",
    "    dtype=g.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following instantiates the top level for the Linear Layer. Prior to implementation, it's important to make a VXM chaining plan. The Groq API Tutorial Guide supplements the information provided here:\n",
    "\n",
    "* nn.matmul() reserves ALUs \\[4] and \\[5] to finish the matmul operation and the data leaves the West MXM, arriving at the VXM on StreamGroups 3E and 4E for the lower and upper byte planes (Byte Plane 0 and Byte Plane 1, respectively).\n",
    "    * Matmul results are combined in ALU\\[4] \n",
    "    * and accumulated in ALU\\[5]\n",
    "\n",
    "Since we're implementing a Linear Layer that includes Bias and ReLU operations, we need two more StreamGroups for the following:\n",
    " * bias -> Add tensor\n",
    " * relu (max) -> Zero tensor (which will check for values less than zero and mask with the zero tensor)\n",
    "\n",
    "The `max` instruction within the ReLU requires a large ALU. The `add` for the `bias_add` can be performed in either a large or small ALU. \n",
    "Thus we'll choose StreamGroup 2E for the add tensor and StreamGroup 5W for the zero tensor. Where E and W represent the direction the data is heading (eastward/westward). We will also account for casting the final result from FP32 to FP16 before returning it to the host which will require a large ALU. So to summarize, we have the following StreamGroups in use:\n",
    "\n",
    " * 2E for the bias-add tensor\n",
    " * 3E for the MXM results from the upper byte plane\n",
    " * 4E for the MXM results from the lower byte plane\n",
    " * 5W as we'll plan on having the zero tensor come from MEM east.\n",
    " * 2W for the result \n",
    "\n",
    "And the following ALUs:\n",
    "\n",
    " * \\[4] MXM combining\n",
    " * \\[5] MXM accumulation (used when Matrix size is larger than 320)\n",
    " * \\[6] Bias Add\n",
    " * \\[7] ReLU\n",
    " * \\[2] Cast FP32 to FP16\n",
    "\n",
    "The VXM chaining plan is shown in the following figure:\n",
    "\n",
    " ![title](LinearVXMChain.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearTopLevel(g.Component):\n",
    "    def __init__(self, weights, bias, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.weights_data = weights\n",
    "        self.bias_data = bias\n",
    "\n",
    "        self.add_alus = [6]     # ALU[6]: see diagram above\n",
    "        self.relu_alus = [7]    # ALU[7]\n",
    "        self.cast_alus = [2]    # ALU[2]\n",
    "\n",
    "        self.matmul = nn.MatMul(\n",
    "            #assign an output StreamGroup to ensure the backend API does not assign something different than the VXM chaining plan\n",
    "            name=\"matmul\", planes=[0, 1], use_vxm_accum=True, out_strm_rq=g.SG4_E[3] \n",
    "        )\n",
    "        self.relu = nn.ReLU(\n",
    "            alus=self.relu_alus,\n",
    "            input_stream_req=g.SG4_E[2],    # bias_add result will be on StreamGroup 2E\n",
    "            output_stream_req=g.SG4_W[2],   # output ReLU result on StreamGroup 2W\n",
    "            zero_stream_req=g.SG4_W[5],     # bring the zero tensor from the east MEM location heading west\n",
    "        )\n",
    "\n",
    "    def build(self, input_mt, time=0, **kwargs):\n",
    "        super().build(**kwargs)\n",
    "\n",
    "        weights_mt = g.from_data(\n",
    "            data=self.weights_data,  \n",
    "            name=\"weights_mt\",\n",
    "            layout=\"H1(W), -1, S16(4-38)\",  #layout for matmul weights\n",
    "        )\n",
    "\n",
    "        bias_mt = g.from_data(\n",
    "            data=self.bias_data, name=\"bias_mt\", layout=\"H1(W), -1, S4\"  # we're using an eastbound stream, so we ensure the bias tensor is in west MEM  \n",
    "        )\n",
    "        bias_st = bias_mt.read(streams=g.SG4_E[2])  # Read bias tensor. \n",
    "\n",
    "        product_st = self.matmul(input_mt, weights_mt, time=0) \n",
    "\n",
    "        # Add bias_add tensor with the matmul results\n",
    "        sum_st = product_st.add(\n",
    "            bias_st,\n",
    "            input_streams=[g.SG4_E[3], g.SG4_E[2]],  # Use StreamGroups 2E and 3E to input to the ALU\n",
    "            output_streams=g.SG4_E[2],               # Output result onto StreamGroup 2E\n",
    "            alus=self.add_alus,     #ALU6\n",
    "        )\n",
    "        relu_st = self.relu(sum_st) # perform ReLU\n",
    "        result_st = g.cast(relu_st, dtype=g.float16, fp16_inf=False, alus=self.cast_alus) #Cast results to FP16\n",
    "\n",
    "        result_mt = result_st.write(name=\"result\", layout=\"H1(W), -1, S2\")  #Write to MEM in West hemisphere. \n",
    "\n",
    "        return result_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = LinearTopLevel(weights_data, bias_data)    # instantiate the top level component\n",
    "result_mt = unit(input_mt, time=0)    # call into the instance of the top level, providing your inputs and time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've instantiated and built the Linear Top Level, we can compile our program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iop_file = g.compile(\n",
    "    base_name=\"linear_test\", result_tensor=[result_mt]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroqView\n",
    "GroqView can be used to view the instructions of your program in the GroqChip. Note: it is expected that you are familiar with the GroqView tool (See \"GroqView User Guide\") for this section of this tutorial. You may skip viewing the program in GroqView and move to the \"Prepare Data for Program\" section.\n",
    "\n",
    "Using the following command, we can create a .json file that can be used to view the program in hardware. This will show:\n",
    "* what instructions occur\n",
    "* where on the chip they take place, as well as \n",
    "* when in time (cycles) each instruction occurs.\n",
    "\n",
    "To launch GroqView, uncomment and run the following command. Remember, you still need to create a tunnel to the server running the GroqView tool to load in another window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.write_visualizer_data(\"linear\")\n",
    "#!groqview linear/visdata.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> before proceeding to the next section, you'll want to stop the above cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on Hardware\n",
    "Program the GroqChip with the binary file of the Matrix Multiply program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program = tsp.create_tsp_runner(iop_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the input data to the GroqChip which will return the results of the linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = program(input_matrix=input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_np(input, weights, bias):\n",
    "    product = np.matmul(input, weights.transpose())\n",
    "    sum = product + bias\n",
    "    result = np.maximum(sum, 0).astype(dtype=np.float16)\n",
    "    return result\n",
    "\n",
    "oracle = linear_np(input_data, weights_data, bias_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear layer with all dimensions set to {}.  Results are: \".format(size))\n",
    "print(np.allclose(oracle, result['result'], rtol=1e-1, atol=1e-1, equal_nan=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
